{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9.822_final_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "PoJAZRMXJ2ls"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVfo4XW4yNuS",
        "outputId": "61cbfddd-6adb-49d5-87a6-9ab78d6616b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed May 11 14:23:56 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.initializers import he_normal, normal\n",
        "\n",
        "# from deepreplay.datasets.parabola import load_data\n",
        "# from deepreplay.callbacks import ReplayData\n",
        "# from deepreplay.replay import Replay\n",
        "# from deepreplay.plot import compose_animations, compose_plots\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n"
      ],
      "metadata": {
        "id": "9PkxZdxVLGe2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = \"/content/drive/MyDrive/9.822\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyrKnKctylIC",
        "outputId": "5c7edd1a-7ab4-4f6b-b66a-b77424701d5b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive/9.822\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAPVRJfa5HAW",
        "outputId": "9c38a25b-9f4c-4d99-f3f6-d1971bcd40f8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/9.822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads the CSV data\n",
        "# spam\n",
        "df_spam = pd.read_csv('spambase.data', header=None)\n",
        "\n",
        "# breast cancer\n",
        "df_breast = pd.read_csv('breast_cancer.csv')\n",
        "df_breast.drop(df_breast.columns[df_breast.columns.str.contains('unnamed', case = False)], axis = 1, inplace = True)\n",
        "\n",
        "# german \n",
        "df_german = pd.read_csv('german.csv')\n",
        "\n",
        "# austrian\n",
        "df_australian = pd.read_csv('australian.csv')\n",
        "\n",
        "# hill-valley\n",
        "df_hill_valley = pd.read_csv('hill-valley.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "d8ATPBEJyi6v",
        "outputId": "5c52d793-b6e9-4001-d4c9-1b3f441cbbf7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0        39.02     36.49     38.20     38.85     39.38     39.74     37.02   \n",
              "1         1.83      1.71      1.77      1.77      1.68      1.78      1.80   \n",
              "2     68177.69  66138.42  72981.88  74304.33  67549.66  69367.34  69169.41   \n",
              "3     44889.06  39191.86  40728.46  38576.36  45876.06  47034.00  46611.43   \n",
              "4         5.70      5.40      5.28      5.38      5.27      5.61      6.00   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1207     13.00     12.87     13.27     13.04     13.19     12.53     14.31   \n",
              "1208     48.66     50.11     48.55     50.43     50.09     49.67     48.95   \n",
              "1209  10160.65   9048.63   8994.94   9514.39   9814.74  10195.24  10031.47   \n",
              "1210     34.81     35.07     34.98     32.37     34.16     34.03     33.31   \n",
              "1211   8489.43   7672.98   9132.14   7985.73   8226.85   8554.28   8838.87   \n",
              "\n",
              "            V8        V9       V10  ...       V92       V93       V94  \\\n",
              "0        39.53     38.81     38.79  ...     36.62     36.92     38.80   \n",
              "1         1.70      1.75      1.78  ...      1.80      1.79      1.77   \n",
              "2     73268.61  74465.84  72503.37  ...  73438.88  71053.35  71112.62   \n",
              "3     37668.32  40980.89  38466.15  ...  42625.67  40684.20  46960.73   \n",
              "4         5.38      5.34      5.87  ...      5.17      5.67      5.60   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "1207     13.33     13.63     14.55  ...     12.48     12.15     13.15   \n",
              "1208     48.65     48.63     48.61  ...     46.93     49.61     47.16   \n",
              "1209  10202.28   9152.99   9591.75  ...   9068.11   9191.80   9275.04   \n",
              "1210     32.48     35.63     32.48  ...     32.76     35.03     32.89   \n",
              "1211   8967.24   8635.14   8544.37  ...   8609.73   9209.48   8496.33   \n",
              "\n",
              "           V95       V96       V97       V98       V99      V100  Class  \n",
              "0        38.52     38.07     36.73     39.46     37.50     39.10      0  \n",
              "1         1.74      1.74      1.80      1.78      1.75      1.69      1  \n",
              "2     74916.48  72571.58  66348.97  71063.72  67404.27  74920.24      1  \n",
              "3     44546.80  45410.53  47139.44  43095.68  40888.34  39615.19      0  \n",
              "4         5.94      5.73      5.22      5.30      5.73      5.91      0  \n",
              "...        ...       ...       ...       ...       ...       ...    ...  \n",
              "1207     12.35     13.58     13.86     12.88     13.87     13.51      1  \n",
              "1208     48.17     47.94     49.81     49.89     47.43     47.77      0  \n",
              "1209   9848.18   9074.17   9601.74  10366.24   8997.60   9305.77      1  \n",
              "1210     31.91     33.85     35.28     32.49     32.83     34.82      1  \n",
              "1211   8724.01   8219.99   8550.86   8679.43   8389.31   8712.80      0  \n",
              "\n",
              "[1212 rows x 101 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a7166b94-e004-4e14-82fa-8d8a4606c36c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>...</th>\n",
              "      <th>V92</th>\n",
              "      <th>V93</th>\n",
              "      <th>V94</th>\n",
              "      <th>V95</th>\n",
              "      <th>V96</th>\n",
              "      <th>V97</th>\n",
              "      <th>V98</th>\n",
              "      <th>V99</th>\n",
              "      <th>V100</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39.02</td>\n",
              "      <td>36.49</td>\n",
              "      <td>38.20</td>\n",
              "      <td>38.85</td>\n",
              "      <td>39.38</td>\n",
              "      <td>39.74</td>\n",
              "      <td>37.02</td>\n",
              "      <td>39.53</td>\n",
              "      <td>38.81</td>\n",
              "      <td>38.79</td>\n",
              "      <td>...</td>\n",
              "      <td>36.62</td>\n",
              "      <td>36.92</td>\n",
              "      <td>38.80</td>\n",
              "      <td>38.52</td>\n",
              "      <td>38.07</td>\n",
              "      <td>36.73</td>\n",
              "      <td>39.46</td>\n",
              "      <td>37.50</td>\n",
              "      <td>39.10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.83</td>\n",
              "      <td>1.71</td>\n",
              "      <td>1.77</td>\n",
              "      <td>1.77</td>\n",
              "      <td>1.68</td>\n",
              "      <td>1.78</td>\n",
              "      <td>1.80</td>\n",
              "      <td>1.70</td>\n",
              "      <td>1.75</td>\n",
              "      <td>1.78</td>\n",
              "      <td>...</td>\n",
              "      <td>1.80</td>\n",
              "      <td>1.79</td>\n",
              "      <td>1.77</td>\n",
              "      <td>1.74</td>\n",
              "      <td>1.74</td>\n",
              "      <td>1.80</td>\n",
              "      <td>1.78</td>\n",
              "      <td>1.75</td>\n",
              "      <td>1.69</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>68177.69</td>\n",
              "      <td>66138.42</td>\n",
              "      <td>72981.88</td>\n",
              "      <td>74304.33</td>\n",
              "      <td>67549.66</td>\n",
              "      <td>69367.34</td>\n",
              "      <td>69169.41</td>\n",
              "      <td>73268.61</td>\n",
              "      <td>74465.84</td>\n",
              "      <td>72503.37</td>\n",
              "      <td>...</td>\n",
              "      <td>73438.88</td>\n",
              "      <td>71053.35</td>\n",
              "      <td>71112.62</td>\n",
              "      <td>74916.48</td>\n",
              "      <td>72571.58</td>\n",
              "      <td>66348.97</td>\n",
              "      <td>71063.72</td>\n",
              "      <td>67404.27</td>\n",
              "      <td>74920.24</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>44889.06</td>\n",
              "      <td>39191.86</td>\n",
              "      <td>40728.46</td>\n",
              "      <td>38576.36</td>\n",
              "      <td>45876.06</td>\n",
              "      <td>47034.00</td>\n",
              "      <td>46611.43</td>\n",
              "      <td>37668.32</td>\n",
              "      <td>40980.89</td>\n",
              "      <td>38466.15</td>\n",
              "      <td>...</td>\n",
              "      <td>42625.67</td>\n",
              "      <td>40684.20</td>\n",
              "      <td>46960.73</td>\n",
              "      <td>44546.80</td>\n",
              "      <td>45410.53</td>\n",
              "      <td>47139.44</td>\n",
              "      <td>43095.68</td>\n",
              "      <td>40888.34</td>\n",
              "      <td>39615.19</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.70</td>\n",
              "      <td>5.40</td>\n",
              "      <td>5.28</td>\n",
              "      <td>5.38</td>\n",
              "      <td>5.27</td>\n",
              "      <td>5.61</td>\n",
              "      <td>6.00</td>\n",
              "      <td>5.38</td>\n",
              "      <td>5.34</td>\n",
              "      <td>5.87</td>\n",
              "      <td>...</td>\n",
              "      <td>5.17</td>\n",
              "      <td>5.67</td>\n",
              "      <td>5.60</td>\n",
              "      <td>5.94</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.22</td>\n",
              "      <td>5.30</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.91</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1207</th>\n",
              "      <td>13.00</td>\n",
              "      <td>12.87</td>\n",
              "      <td>13.27</td>\n",
              "      <td>13.04</td>\n",
              "      <td>13.19</td>\n",
              "      <td>12.53</td>\n",
              "      <td>14.31</td>\n",
              "      <td>13.33</td>\n",
              "      <td>13.63</td>\n",
              "      <td>14.55</td>\n",
              "      <td>...</td>\n",
              "      <td>12.48</td>\n",
              "      <td>12.15</td>\n",
              "      <td>13.15</td>\n",
              "      <td>12.35</td>\n",
              "      <td>13.58</td>\n",
              "      <td>13.86</td>\n",
              "      <td>12.88</td>\n",
              "      <td>13.87</td>\n",
              "      <td>13.51</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1208</th>\n",
              "      <td>48.66</td>\n",
              "      <td>50.11</td>\n",
              "      <td>48.55</td>\n",
              "      <td>50.43</td>\n",
              "      <td>50.09</td>\n",
              "      <td>49.67</td>\n",
              "      <td>48.95</td>\n",
              "      <td>48.65</td>\n",
              "      <td>48.63</td>\n",
              "      <td>48.61</td>\n",
              "      <td>...</td>\n",
              "      <td>46.93</td>\n",
              "      <td>49.61</td>\n",
              "      <td>47.16</td>\n",
              "      <td>48.17</td>\n",
              "      <td>47.94</td>\n",
              "      <td>49.81</td>\n",
              "      <td>49.89</td>\n",
              "      <td>47.43</td>\n",
              "      <td>47.77</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1209</th>\n",
              "      <td>10160.65</td>\n",
              "      <td>9048.63</td>\n",
              "      <td>8994.94</td>\n",
              "      <td>9514.39</td>\n",
              "      <td>9814.74</td>\n",
              "      <td>10195.24</td>\n",
              "      <td>10031.47</td>\n",
              "      <td>10202.28</td>\n",
              "      <td>9152.99</td>\n",
              "      <td>9591.75</td>\n",
              "      <td>...</td>\n",
              "      <td>9068.11</td>\n",
              "      <td>9191.80</td>\n",
              "      <td>9275.04</td>\n",
              "      <td>9848.18</td>\n",
              "      <td>9074.17</td>\n",
              "      <td>9601.74</td>\n",
              "      <td>10366.24</td>\n",
              "      <td>8997.60</td>\n",
              "      <td>9305.77</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1210</th>\n",
              "      <td>34.81</td>\n",
              "      <td>35.07</td>\n",
              "      <td>34.98</td>\n",
              "      <td>32.37</td>\n",
              "      <td>34.16</td>\n",
              "      <td>34.03</td>\n",
              "      <td>33.31</td>\n",
              "      <td>32.48</td>\n",
              "      <td>35.63</td>\n",
              "      <td>32.48</td>\n",
              "      <td>...</td>\n",
              "      <td>32.76</td>\n",
              "      <td>35.03</td>\n",
              "      <td>32.89</td>\n",
              "      <td>31.91</td>\n",
              "      <td>33.85</td>\n",
              "      <td>35.28</td>\n",
              "      <td>32.49</td>\n",
              "      <td>32.83</td>\n",
              "      <td>34.82</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1211</th>\n",
              "      <td>8489.43</td>\n",
              "      <td>7672.98</td>\n",
              "      <td>9132.14</td>\n",
              "      <td>7985.73</td>\n",
              "      <td>8226.85</td>\n",
              "      <td>8554.28</td>\n",
              "      <td>8838.87</td>\n",
              "      <td>8967.24</td>\n",
              "      <td>8635.14</td>\n",
              "      <td>8544.37</td>\n",
              "      <td>...</td>\n",
              "      <td>8609.73</td>\n",
              "      <td>9209.48</td>\n",
              "      <td>8496.33</td>\n",
              "      <td>8724.01</td>\n",
              "      <td>8219.99</td>\n",
              "      <td>8550.86</td>\n",
              "      <td>8679.43</td>\n",
              "      <td>8389.31</td>\n",
              "      <td>8712.80</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1212 rows × 101 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7166b94-e004-4e14-82fa-8d8a4606c36c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a7166b94-e004-4e14-82fa-8d8a4606c36c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a7166b94-e004-4e14-82fa-8d8a4606c36c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification "
      ],
      "metadata": {
        "id": "oKWQH2STJ9RX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classifiers**"
      ],
      "metadata": {
        "id": "UrGBHDvGj1fY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainer(classifier, x_train, y_train, x_test, y_test):\n",
        "  classifier.fit(x_train, y_train)\n",
        "  y_pred = classifier.predict(x_test)\n",
        "  return accuracy_score(y_test, y_pred)\n",
        "\n",
        "#Using Logistic Regression Algorithm to the Training Set\n",
        "classifier_lr = LogisticRegression(random_state = 0)\n",
        "\n",
        "#Using KNeighborsClassifier Method of neighbors class to use Nearest Neighbor algorithm\n",
        "classifier_knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
        "\n",
        "#Using SVC method of svm class to use Support Vector Machine Algorithm\n",
        "classifier_svc_l = SVC(kernel = 'linear', random_state = 0)\n",
        "\n",
        "#Using SVC method of svm class to use Kernel SVM Algorithm\n",
        "classifier_svc_rbf = SVC(kernel = 'rbf', random_state = 0)\n",
        "\n",
        "#Using GaussianNB method of naïve_bayes class to use Naïve Bayes Algorithm\n",
        "classifier_gnb = GaussianNB()\n",
        "\n",
        "#Using DecisionTreeClassifier of tree class to use Decision Tree Algorithm\n",
        "classifier_cart = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
        "\n",
        "#Using RandomForestClassifier method of ensemble class to use Random Forest Classification algorithm\n",
        "classifier_rf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
        "\n",
        "# using Perceptron method \n",
        "classifier_perceptron = Perceptron(tol=1e-3, random_state=0)\n",
        "\n",
        "# using MLP classifier\n",
        "classifier_mlp = MLPClassifier(random_state=0, max_iter=300, verbose=False, tol=1e-2)"
      ],
      "metadata": {
        "id": "U9YQ_sp5jrrn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Net**"
      ],
      "metadata": {
        "id": "KoXe3-hejwlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nn_trainer(input_dim, X_train, y_train, X_test, y_test):\n",
        "    \n",
        "  he_initializer = he_normal(seed=42)\n",
        "  normal_initializer = normal(seed=42)\n",
        "\n",
        "  model = Sequential()\n",
        "  # Hidden layer with 10 units, taking the 57 features as inputs\n",
        "  model.add(Dense(input_dim=input_dim,\n",
        "                  units=10,\n",
        "                  kernel_initializer=he_initializer,\n",
        "                  activation='tanh'))\n",
        "\n",
        "  # Added layer to allow plotting the feature space\n",
        "  # It has 2 units and uses a LINEAR activation, so the network will also learn the\n",
        "  # mapping from 10-dimensions to 2-dimensions\n",
        "  model.add(Dense(units=2,\n",
        "                  kernel_initializer=normal_initializer,\n",
        "                  activation='linear',\n",
        "                  name='hidden'))\n",
        "\n",
        "  # Typical output layer for binary classification\n",
        "  model.add(Dense(units=1,\n",
        "                  kernel_initializer=normal_initializer,\n",
        "                  activation='sigmoid',\n",
        "                  name='output'))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['acc'])\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  # model.fit(X, y, epochs=100, batch_size=16, callbacks=[replaydata])\n",
        "  # model.fit(X_train, y_train, epochs=20, batch_size=16)\n",
        "  # results = model.evaluate(X_test, y_test)\n",
        "  # print(\"test loss, test acc:\", results)"
      ],
      "metadata": {
        "id": "TxrHGOT5jvfh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SpamBase**"
      ],
      "metadata": {
        "id": "sFzymToRKDT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group_name = 'spam'\n",
        "# The first 57 columns are features\n",
        "# The last column has the correct labels (targets)\n",
        "X, Y = df_spam.iloc[:, :57].values, df_spam.iloc[:, 57].values\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,\n",
        "                                                    random_state = 0)\n",
        "\n",
        "# Scale the features, as the original values have wide ranges\n",
        "X = StandardScaler().fit_transform(X)\n",
        "#replaydata = ReplayData(X, y, filename='spambase_dataset.h5', group_name=group_name)\n",
        "# nn_trainer(57, X_train, y_train, X_test, y_test)\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "# evaluation \n",
        "print(trainer(classifier_rf, X_train, Y_train, X_test, Y_test))\n",
        "print(cross_val_score(estimator = classifier_rf, X = X_train, y = Y_train, cv = 10).mean())\n",
        "\n",
        "mapper_spam = {'Spam': [X, Y]}\n",
        "df_spam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "whLCRkOGyrDB",
        "outputId": "b3f6b58d-3fe5-4892-9d82-d241092398c1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9b8a1dc33620>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# The first 57 columns are features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# The last column has the correct labels (targets)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_spam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m57\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_spam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m57\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,\n\u001b[1;32m      6\u001b[0m                                                     random_state = 0)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_spam' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Breast Cancer Wisconsin (Diagnostic)**"
      ],
      "metadata": {
        "id": "6LqV3Fa-KH_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_breast.iloc[:, 2:32].values\n",
        "Y = df_breast.iloc[:, 1].values\n",
        "\n",
        "#Encoding categorical data values\n",
        "labelencoder_Y = LabelEncoder()\n",
        "Y = labelencoder_Y.fit_transform(Y)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)\n",
        "\n",
        "#Feature Scaling\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "# evaluation\n",
        "print(trainer(classifier_svc_l, X_train, Y_train, X_test, Y_test))\n",
        "print(cross_val_score(estimator = classifier_svc_l, X = X_train, y = Y_train, cv = 10).mean())\n",
        "mapper_breast = {'Breast': [X, Y]}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3p7RCCerKIxF",
        "outputId": "19f07191-bbcb-4530-d13c-fe531c427804"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.972027972027972\n",
            "0.9743632336655592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**German Credit Dataset**"
      ],
      "metadata": {
        "id": "inNV7IWSKck0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_german.iloc[:, 1:]\n",
        "Y = df_german.iloc[:, 0]\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 23, stratify=Y)\n",
        "# normalzie\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "# evaluation\n",
        "# print(trainer(Perceptron(tol=1e-3, random_state=0), X_train, Y_train, X_test, Y_test))\n",
        "print(cross_val_score(estimator = classifier_svc_l, X = X_train, y = Y_train, cv = 10).mean())\n",
        "\n",
        "mapper_german = {'German': [X, Y]}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsJc2iVv7Fn2",
        "outputId": "43147e87-5daa-426a-f0f3-fc4a63fbe2ac"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Australian Credit Approval Dataset**"
      ],
      "metadata": {
        "id": "4nX_YXs4KkfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_australian.iloc[:, 0:14]\n",
        "Y = df_australian.iloc[:, 14]\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 23, stratify=Y)\n",
        "# normalize\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "# evaluation\n",
        "print(trainer(classifier_svc_l, X_train, Y_train, X_test, Y_test))\n",
        "print(cross_val_score(estimator = classifier_svc_l, X = X_train, y = Y_train, cv = 10).mean())\n",
        "\n",
        "mapper_australian = {'Australian': [X, Y]}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpE0XqMuKuex",
        "outputId": "aa27e929-6303-42dc-dab0-e273baf30ea9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8840579710144928\n",
            "0.8459090909090909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hill-Valley Dataset**"
      ],
      "metadata": {
        "id": "i68kKJtjKu6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_hill_valley.iloc[:, 0:100]\n",
        "Y = df_hill_valley.iloc[:, 100]\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 23, stratify=Y)\n",
        "# normalize\n",
        "X_train = normalize(X_train)\n",
        "X_test = normalize(X_test)\n",
        "\n",
        "# evaluation\n",
        "print(trainer(classifier_svc_rbf, X_train, Y_train, X_test, Y_test))\n",
        "print(cross_val_score(estimator = classifier_svc_rbf, X = X_train, y = Y_train, cv = 10).mean())\n",
        "\n",
        "mapper_hill_valley = {'Hill-Valley': [X, Y]}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCRAGof5KyiK",
        "outputId": "bfbad182-cf4a-490d-91fd-92e45b114744"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9876543209876543\n",
            "0.9907216494845361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mapper = {}\n",
        "mapper.update(mapper_spam)\n",
        "mapper.update(mapper_breast)\n",
        "mapper.update(mapper_german)\n",
        "mapper.update(mapper_australian)\n",
        "mapper.update(mapper_hill_valley)"
      ],
      "metadata": {
        "id": "gkrrCrmVZCOs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Heuristic Machine Truth Serum (HMTS)"
      ],
      "metadata": {
        "id": "7gd_lqfOCx9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifiers(mapper):\n",
        "  for k, vals in mapper.items():\n",
        "    vals[0].fit(vals[1], vals[3])\n",
        "    print('Finished trainining ' + k)\n",
        "  return mapper\n",
        "\n",
        "def peer_prediction(classifier, classifier_result, x, mapper):\n",
        "  peer_prediction_estimation = 0 \n",
        "  for k, vals in mapper.items():\n",
        "    if k == classifier:\n",
        "      continue\n",
        "    result = vals[0].predict(x)\n",
        "    if classifier_result == result:\n",
        "      peer_prediction_estimation += 1\n",
        "  \n",
        "  return peer_prediction_estimation / len(mapper.keys())\n",
        "\n",
        "def calc_posterior_and_majority(mapper, answer, x, Large):\n",
        "  posterior = 0\n",
        "  counter_0 = 0\n",
        "  counter_1 = 0\n",
        "  majority = None\n",
        "  high_disagreement = False\n",
        "  for k, vals in mapper.items():\n",
        "    pred = vals[0].predict(x)\n",
        "    if pred == answer:\n",
        "      posterior += 1\n",
        "    if pred == 0:\n",
        "      counter_0 += 1\n",
        "    else:\n",
        "      counter_1 += 1\n",
        "\n",
        "  if counter_0 > counter_1:\n",
        "    majority = 0\n",
        "  else:\n",
        "    majority = 1\n",
        "\n",
        "  posterior = posterior / len(mapper.keys())\n",
        "  if Large:\n",
        "    if counter_0 == 8 or counter_0 == 7:\n",
        "      high_disagreement = True\n",
        "  else:\n",
        "    if counter_0 == 2 or counter_0 == 3:\n",
        "      high_disagreement = True\n",
        "  return posterior, majority, high_disagreement\n",
        "    \n",
        "def HMTS(mapper, one_regressor=False, Large=True):\n",
        "  # train each classifier\n",
        "  mapper = train_classifiers(mapper)\n",
        "\n",
        "  # for each classifier\n",
        "  new_mapper = mapper.copy()\n",
        "  for k, vals in mapper.items():\n",
        "    # for each data point in X_train\n",
        "    print('Working on classifier ' + k)\n",
        "    peer_prediction_0_x = []\n",
        "    peer_prediction_0_y = []\n",
        "    peer_prediction_1_x = []\n",
        "    peer_prediction_1_y = []\n",
        "    for i in vals[1]:\n",
        "      # compute peer prediction information\n",
        "      x = i.reshape(1, -1)\n",
        "      pred = vals[0].predict(x)\n",
        "      peer_pred = peer_prediction(k, pred, x, mapper)\n",
        "      if one_regressor:\n",
        "        peer_prediction_0_x.append(i)\n",
        "        peer_prediction_0_y.append(peer_pred)\n",
        "      else:\n",
        "        if pred == 0:\n",
        "          peer_prediction_0_x.append(i)\n",
        "          peer_prediction_0_y.append(peer_pred)\n",
        "        elif pred == 1:\n",
        "          peer_prediction_1_x.append(i)\n",
        "          peer_prediction_1_y.append(peer_pred)\n",
        "\n",
        "    assert len(peer_prediction_0_x) == len(peer_prediction_0_y), \"lengths must be equal\"\n",
        "    if not one_regressor:\n",
        "      assert len(peer_prediction_1_x) == len(peer_prediction_1_y), \"lengths must be equal\"\n",
        "      assert len(peer_prediction_0_x) + len(peer_prediction_1_x) == len(vals[1]), \"lengths must sum to len(X_train)\"\n",
        "\n",
        "    if one_regressor:\n",
        "      belief_regressor_0 = RandomForestRegressor(n_estimators = 100, criterion = 'squared_error', random_state = 0).fit(\n",
        "          peer_prediction_0_x, peer_prediction_0_y)\n",
        "      new_mapper[k].extend([belief_regressor_0, peer_prediction_0_x, peer_prediction_0_y])\n",
        "\n",
        "    else:\n",
        "      if len(peer_prediction_0_x) != 0:\n",
        "        belief_regressor_0 = RandomForestRegressor(n_estimators = 100, criterion = 'squared_error', random_state = 0).fit(peer_prediction_0_x, peer_prediction_0_y)\n",
        "      else:\n",
        "        belief_regressor_0 = RandomForestRegressor(n_estimators = 100, criterion = 'squared_error', random_state = 0)\n",
        "      if len(peer_prediction_1_x) != 0:\n",
        "        belief_regressor_1 = RandomForestRegressor(n_estimators = 100, criterion = 'squared_error', random_state = 0).fit(peer_prediction_1_x, peer_prediction_1_y)\n",
        "      else:\n",
        "        belief_regressor_1 = RandomForestRegressor(n_estimators = 100, criterion = 'squared_error', random_state = 0)\n",
        "\n",
        "      new_mapper[k].extend([belief_regressor_0, peer_prediction_0_x, peer_prediction_0_y, \n",
        "                            belief_regressor_1, peer_prediction_1_x, peer_prediction_1_y])\n",
        "    \n",
        "  # evaluation\n",
        "  test_data = new_mapper['RandomForest_100'][2]\n",
        "  right_answers = new_mapper['RandomForest_100'][4].reset_index()\n",
        "  hmts_answers = []\n",
        "  majority_answers = []\n",
        "  ground_truth = []\n",
        "  for inx, x in enumerate(test_data):\n",
        "    x_transformed = x.reshape(1, -1)\n",
        "    prior_0 = 0\n",
        "    posterior_0 = 0\n",
        "    for k, vals in new_mapper.items():\n",
        "      pred = vals[0].predict(x_transformed)\n",
        "      if one_regressor:\n",
        "        peer_pred = vals[6].predict(x_transformed)\n",
        "      else:\n",
        "        if pred == 0:\n",
        "          peer_pred = vals[6].predict(x_transformed)\n",
        "        elif pred == 1:\n",
        "          peer_pred = 1 - vals[9].predict(x_transformed)\n",
        "      prior_0 += peer_pred\n",
        "\n",
        "    prior_0 = prior_0 / len(new_mapper.keys())\n",
        "    posterior_0, majority, high_disagreement = calc_posterior_and_majority(new_mapper, 0, x_transformed, Large)\n",
        "    if high_disagreement:\n",
        "      if posterior_0 > prior_0:\n",
        "        hmts_answers.append(0)\n",
        "      else:\n",
        "        hmts_answers.append(1)\n",
        "      majority_answers.append(majority)\n",
        "      ground_truth.append(right_answers.iloc[inx, 1])\n",
        "    else:\n",
        "      hmts_answers.append(majority)\n",
        "      majority_answers.append(majority)\n",
        "      ground_truth.append(right_answers.iloc[inx, 1])\n",
        "  print('Done')\n",
        "  return hmts_answers, majority_answers, ground_truth"
      ],
      "metadata": {
        "id": "AHX93IMrCybo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discriminative Machine Truth Serum (DMTS)"
      ],
      "metadata": {
        "id": "h4iXwNyplaoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_majority(mapper, x, Large=True):\n",
        "  counter_0 = 0\n",
        "  counter_1 = 0\n",
        "  majority_0 = None\n",
        "  high_disagreement = False\n",
        "  for k, vals in mapper.items():\n",
        "    pred = vals[0].predict(x)\n",
        "    if pred == 0:\n",
        "      counter_0 += 1\n",
        "    elif pred == 1:\n",
        "      counter_1 += 1\n",
        "  if counter_0 > counter_1:\n",
        "    majority_0 = 0\n",
        "  else:\n",
        "    majority_0 = 1\n",
        "  \n",
        "  if Large:\n",
        "    if counter_0 == 8 or counter_0 == 7:\n",
        "      high_disagreement = True\n",
        "  else:\n",
        "    if counter_0 == 2 or counter_0 == 3:\n",
        "      high_disagreement = True\n",
        "  return majority_0, high_disagreement\n",
        "  \n",
        "\n",
        "def DMTS(mapper, Large=True):\n",
        "  # train each classifier\n",
        "  mapper = train_classifiers(mapper)\n",
        "  train_data = mapper['RandomForest_100'][1]\n",
        "  y_train = mapper['RandomForest_100'][3].reset_index()\n",
        "  majority_right = []\n",
        "  for inx, x in enumerate(train_data):\n",
        "    x_transformed = x.reshape(1, -1)\n",
        "    majority, high_disagreement = calc_majority(mapper, x_transformed)\n",
        "    right_answer = y_train.iloc[inx, 1]\n",
        "    if majority == right_answer:\n",
        "      majority_right.append(0)\n",
        "    else:\n",
        "      majority_right.append(1)\n",
        "\n",
        "  # fit majority checker classifier\n",
        "  classifier_rf = RandomForestClassifier(n_estimators = 15, criterion = 'entropy', random_state = 0)\n",
        "  classifier_rf.fit(train_data, majority_right)\n",
        "\n",
        "  test_data = mapper['RandomForest_100'][2]\n",
        "  y_test = mapper['RandomForest_100'][4].reset_index()\n",
        "  ground_truth = []\n",
        "  majority_answers = []\n",
        "  dmts_answers = [] \n",
        "  for inx, x in enumerate(test_data):\n",
        "    x_transformed = x.reshape(1, -1)\n",
        "    pred = classifier_rf.predict(x_transformed)\n",
        "    majority, high_disagreement = calc_majority(mapper, x_transformed, Large)\n",
        "    if high_disagreement:\n",
        "      # print('high disagreement encountered')\n",
        "      majority_answers.append(majority)\n",
        "      ground_truth.append(y_test.iloc[inx, 1])\n",
        "      if pred == 0:\n",
        "        dmts_answers.append(majority)\n",
        "      else:\n",
        "        if majority == 0:\n",
        "          dmts_answers.append(1)\n",
        "        else:\n",
        "          dmts_answers.append(0)\n",
        "    else:\n",
        "      dmts_answers.append(majority)\n",
        "      majority_answers.append(majority)\n",
        "      ground_truth.append(y_test.iloc[inx, 1])\n",
        "  return dmts_answers, majority_answers, ground_truth"
      ],
      "metadata": {
        "id": "umHOrmv-laCt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Datasets"
      ],
      "metadata": {
        "id": "nJy3m7pivh3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_initializer(X_train, X_test, Y_train, Y_test, Y, Large=True):\n",
        "  data_mapper = None\n",
        "  if Large:\n",
        "    data_mapper = {'Decision_Tree': [DecisionTreeClassifier(criterion = 'entropy', random_state = 0), X_train, X_test, Y_train, Y_test, Y], \n",
        "                      'Naive_Bayes': [GaussianNB(), X_train, X_test, Y_train, Y_test, Y],\n",
        "                      'Perceptron': [Perceptron(tol=1e-5, random_state=0), X_train, X_test, Y_train, Y_test, Y], \n",
        "                      'Logistic_500': [LogisticRegression(random_state = 0, max_iter=500), X_train, X_test, Y_train, Y_test, Y], \n",
        "                      'Logistic_400': [LogisticRegression(random_state = 0, max_iter=400), X_train, X_test, Y_train, Y_test, Y],\n",
        "                      'Logistic_300': [LogisticRegression(random_state = 0, max_iter=300), X_train, X_test, Y_train, Y_test, Y], \n",
        "                      'RandomForest_15': [RandomForestClassifier(n_estimators = 15, criterion = 'entropy', random_state = 0), \n",
        "                                          X_train, X_test, Y_train, Y_test, Y], \n",
        "                      'RandomForest_50': [RandomForestClassifier(n_estimators = 50, criterion = 'entropy', random_state = 0), \n",
        "                                          X_train, X_test, Y_train, Y_test, Y], \n",
        "                      'RandomForest_100': [RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0), \n",
        "                                          X_train, X_test, Y_train, Y_test, Y], \n",
        "                      'SVM_rbf': [SVC(kernel = 'rbf', random_state = 0), X_train, X_test, Y_train, Y_test, Y], \n",
        "                      'SVM_linear': [SVC(kernel = 'linear', random_state = 0), X_train, X_test, Y_train, Y_test, Y], \n",
        "                      'SVM_poly': [SVC(kernel = 'poly', random_state = 0), X_train, X_test, Y_train, Y_test, Y],\n",
        "                      'MLP_300': [MLPClassifier(random_state=0, max_iter=300, verbose=False, tol=1e-2), X_train, X_test, Y_train, Y_test, Y],\n",
        "                      'MLP_400': [MLPClassifier(random_state=0, max_iter=400, verbose=False, tol=1e-2), X_train, X_test, Y_train, Y_test, Y],\n",
        "                      'MLP_500': [MLPClassifier(random_state=0, max_iter=500, verbose=False, tol=1e-2), X_train, X_test, Y_train, Y_test, Y]}\n",
        "  else:\n",
        "    data_mapper = {'Perceptron_1': [Perceptron(tol=1e-3, random_state=0), X_train, X_test, Y_train, Y_test, Y], \n",
        "                    'RandomForest_100': [RandomForestClassifier(n_estimators = 50, criterion = 'entropy', random_state = 0), \n",
        "                                        X_train, X_test, Y_train, Y_test, Y], \n",
        "                    'SVM_1': [SVC(kernel = 'rbf', random_state = 0), X_train, X_test, Y_train, Y_test, Y], \n",
        "                    'Logistic_1': [LogisticRegression(random_state = 0, max_iter=300), X_train, X_test, Y_train, Y_test, Y], \n",
        "                    'MLP_300': [MLPClassifier(random_state=0, max_iter=300, verbose=False, tol=1e-2), X_train, X_test, Y_train, Y_test, Y]}\n",
        "  return data_mapper\n",
        "\n",
        "\n",
        "def train_mts(mapper):\n",
        "  results = {}\n",
        "  for mapper_name in mapper.keys():\n",
        "    print(\"-------------------------------------------------------------------\")\n",
        "    print('Working on dataset', mapper_name)\n",
        "    Y_06 = []\n",
        "    Y_08 = []\n",
        "    Y_1 = []\n",
        "    for inx, val in enumerate(mapper[mapper_name][1]):\n",
        "      if val == 0:\n",
        "        new_val_06 = random.choices([0, 1], weights = [0.94, 0.06])\n",
        "        new_val_08 = random.choices([0, 1], weights = [0.92, 0.08])\n",
        "        new_val_1 = random.choices([0, 1], weights = [0.9, 0.1])\n",
        "      elif val == 1:\n",
        "        new_val_06 = random.choices([1, 0], weights = [0.94, 0.06])\n",
        "        new_val_08 = random.choices([1, 0], weights = [0.92, 0.08])\n",
        "        new_val_1 = random.choices([1, 0], weights = [0.9, 0.1])\n",
        "\n",
        "      Y_06.append(new_val_06[0])\n",
        "      Y_08.append(new_val_08[0])\n",
        "      Y_1.append(new_val_1[0])\n",
        "\n",
        "    Y_06 = pd.Series(Y_06)\n",
        "    Y_08 = pd.Series(Y_08)\n",
        "    Y_1 = pd.Series(Y_1)\n",
        "\n",
        "    X = mapper[mapper_name][0]\n",
        "    Y = pd.Series(mapper[mapper_name][1])\n",
        "\n",
        "    X_train, X_test, Y_train_06, Y_test_06 = train_test_split(X, Y_06, test_size = 0.2, random_state = 23, stratify=Y_06)\n",
        "    X_train, X_test, Y_train_08, Y_test_08 = train_test_split(X, Y_08, test_size = 0.2, random_state = 23, stratify=Y_08)\n",
        "    X_train, X_test, Y_train_1, Y_test_1 = train_test_split(X, Y_1, test_size = 0.2, random_state = 23, stratify=Y_1)\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 23, stratify=Y)\n",
        "    # normalize\n",
        "    if mapper_name == 'Hill-Valley':\n",
        "      X_train = normalize(X_train)\n",
        "      X_test = normalize(X_test)\n",
        "    else:\n",
        "      sc = StandardScaler()\n",
        "      X_train = sc.fit_transform(X_train)\n",
        "      X_test = sc.transform(X_test)\n",
        "\n",
        "    # data_mapper = {'Perceptron_06': [Perceptron(tol=1e-3, random_state=0), X_train, X_test, Y_train_06, Y_test_06, Y], \n",
        "    #                     'Perceptron_08': [Perceptron(tol=1e-3, random_state=0), X_train, X_test, Y_train_08, Y_test_08, Y],\n",
        "    #                     'Perceptron_1': [Perceptron(tol=1e-3, random_state=0), X_train, X_test, Y_train_1, Y_test_1, Y], \n",
        "    #                     'Logistic_06': [LogisticRegression(random_state = 0, max_iter=500), X_train, X_test, Y_train_06, Y_test_06, Y], \n",
        "    #                     'Logistic_08': [LogisticRegression(random_state = 0, max_iter=500), X_train, X_test, Y_train_08, Y_test_08, Y],\n",
        "    #                     'Logistic_1': [LogisticRegression(random_state = 0, max_iter=500), X_train, X_test, Y_train_1, Y_test_1, Y], \n",
        "    #                     'RandomForest_06': [RandomForestClassifier(n_estimators = 15, criterion = 'entropy', random_state = 0), \n",
        "    #                                         X_train, X_test, Y_train_06, Y_test_06, Y], \n",
        "    #                     'RandomForest_08': [RandomForestClassifier(n_estimators = 15, criterion = 'entropy', random_state = 0), \n",
        "    #                                         X_train, X_test, Y_train_08, Y_test_08, Y], \n",
        "    #                     'RandomForest_1': [RandomForestClassifier(n_estimators = 15, criterion = 'entropy', random_state = 0), \n",
        "    #                                         X_train, X_test, Y_train_1, Y_test_1, Y], \n",
        "    #                     'SVM_06': [SVC(kernel = 'rbf', random_state = 0), X_train, X_test, Y_train_06, Y_test_06, Y], \n",
        "    #                     'SVM_08': [SVC(kernel = 'rbf', random_state = 0), X_train, X_test, Y_train_08, Y_test_08, Y], \n",
        "    #                     'SVM_1': [SVC(kernel = 'rbf', random_state = 0), X_train, X_test, Y_train_1, Y_test_1, Y],\n",
        "    #                     'MLP_06': [MLPClassifier(random_state=0, max_iter=300, verbose=False, tol=1e-2), X_train, X_test, Y_train_06, Y_test_06, Y],\n",
        "    #                     'MLP_08': [MLPClassifier(random_state=0, max_iter=300, verbose=False, tol=1e-2), X_train, X_test, Y_train_08, Y_test_08, Y],\n",
        "    #                     'MLP_1': [MLPClassifier(random_state=0, max_iter=300, verbose=False, tol=1e-2), X_train, X_test, Y_train_1, Y_test_1, Y]}\n",
        "\n",
        "\n",
        "    data_mapper = data_initializer(X_train, X_test, Y_train, Y_test, Y, Large=True)\n",
        "    hmts, majority, ground_truth = HMTS(data_mapper, one_regressor=False, Large=True)\n",
        "    print('HMTS Score:', accuracy_score(ground_truth, hmts))\n",
        "    print('Majority Score:', accuracy_score(ground_truth, majority))\n",
        "\n",
        "    data_mapper = data_initializer(X_train, X_test, Y_train, Y_test, Y, Large=True)\n",
        "    dmts, majority, ground_truth = DMTS(data_mapper, Large=True)\n",
        "    print('DMTS Score:', accuracy_score(ground_truth, dmts))\n",
        "    print('Majority Score:', accuracy_score(ground_truth, majority))\n",
        "    print('Random Forest Score:', accuracy_score(Y_test, RandomForestClassifier(n_estimators = 15, criterion = 'entropy', random_state = 0).fit(\n",
        "        X_train, Y_train).predict(X_test)))\n",
        "    print('Adaboost Score:', accuracy_score(Y_test, AdaBoostClassifier(n_estimators=15).fit(\n",
        "        X_train, Y_train).predict(X_test)))\n",
        "    # results[mapper_name] = [hmts, dmts, majority, ground_truth]\n",
        "  return results"
      ],
      "metadata": {
        "id": "95wzg-0Fvk_m"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = train_mts(mapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdeP56obYlAX",
        "outputId": "ad0480f4-047b-4931-d3af-4462a446c2be"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------------\n",
            "Working on dataset Spam\n",
            "Finished trainining Decision_Tree\n",
            "Finished trainining Naive_Bayes\n",
            "Finished trainining Perceptron\n",
            "Finished trainining Logistic_500\n",
            "Finished trainining Logistic_400\n",
            "Finished trainining Logistic_300\n",
            "Finished trainining RandomForest_15\n",
            "Finished trainining RandomForest_50\n",
            "Finished trainining RandomForest_100\n",
            "Finished trainining SVM_rbf\n",
            "Finished trainining SVM_linear\n",
            "Finished trainining SVM_poly\n",
            "Finished trainining MLP_300\n",
            "Finished trainining MLP_400\n",
            "Finished trainining MLP_500\n",
            "Working on classifier Decision_Tree\n",
            "Working on classifier Naive_Bayes\n",
            "Working on classifier Perceptron\n",
            "Working on classifier Logistic_500\n",
            "Working on classifier Logistic_400\n",
            "Working on classifier Logistic_300\n",
            "Working on classifier RandomForest_15\n",
            "Working on classifier RandomForest_50\n",
            "Working on classifier RandomForest_100\n",
            "Working on classifier SVM_rbf\n",
            "Working on classifier SVM_linear\n",
            "Working on classifier SVM_poly\n",
            "Working on classifier MLP_300\n",
            "Working on classifier MLP_400\n",
            "Working on classifier MLP_500\n",
            "Done\n",
            "HMTS Score: 0.9489685124864278\n",
            "Majority Score: 0.9478827361563518\n",
            "Finished trainining Decision_Tree\n",
            "Finished trainining Naive_Bayes\n",
            "Finished trainining Perceptron\n",
            "Finished trainining Logistic_500\n",
            "Finished trainining Logistic_400\n",
            "Finished trainining Logistic_300\n",
            "Finished trainining RandomForest_15\n",
            "Finished trainining RandomForest_50\n",
            "Finished trainining RandomForest_100\n",
            "Finished trainining SVM_rbf\n",
            "Finished trainining SVM_linear\n",
            "Finished trainining SVM_poly\n",
            "Finished trainining MLP_300\n",
            "Finished trainining MLP_400\n",
            "Finished trainining MLP_500\n",
            "DMTS Score: 0.9489685124864278\n",
            "Majority Score: 0.9478827361563518\n",
            "Random Forest Score: 0.9554831704668838\n",
            "Adaboost Score: 0.9250814332247557\n",
            "-------------------------------------------------------------------\n",
            "Working on dataset Breast\n",
            "Finished trainining Decision_Tree\n",
            "Finished trainining Naive_Bayes\n",
            "Finished trainining Perceptron\n",
            "Finished trainining Logistic_500\n",
            "Finished trainining Logistic_400\n",
            "Finished trainining Logistic_300\n",
            "Finished trainining RandomForest_15\n",
            "Finished trainining RandomForest_50\n",
            "Finished trainining RandomForest_100\n",
            "Finished trainining SVM_rbf\n",
            "Finished trainining SVM_linear\n",
            "Finished trainining SVM_poly\n",
            "Finished trainining MLP_300\n",
            "Finished trainining MLP_400\n",
            "Finished trainining MLP_500\n",
            "Working on classifier Decision_Tree\n",
            "Working on classifier Naive_Bayes\n",
            "Working on classifier Perceptron\n",
            "Working on classifier Logistic_500\n",
            "Working on classifier Logistic_400\n",
            "Working on classifier Logistic_300\n",
            "Working on classifier RandomForest_15\n",
            "Working on classifier RandomForest_50\n",
            "Working on classifier RandomForest_100\n",
            "Working on classifier SVM_rbf\n",
            "Working on classifier SVM_linear\n",
            "Working on classifier SVM_poly\n",
            "Working on classifier MLP_300\n",
            "Working on classifier MLP_400\n",
            "Working on classifier MLP_500\n",
            "Done\n",
            "HMTS Score: 0.9736842105263158\n",
            "Majority Score: 0.9824561403508771\n",
            "Finished trainining Decision_Tree\n",
            "Finished trainining Naive_Bayes\n",
            "Finished trainining Perceptron\n",
            "Finished trainining Logistic_500\n",
            "Finished trainining Logistic_400\n",
            "Finished trainining Logistic_300\n",
            "Finished trainining RandomForest_15\n",
            "Finished trainining RandomForest_50\n",
            "Finished trainining RandomForest_100\n",
            "Finished trainining SVM_rbf\n",
            "Finished trainining SVM_linear\n",
            "Finished trainining SVM_poly\n",
            "Finished trainining MLP_300\n",
            "Finished trainining MLP_400\n",
            "Finished trainining MLP_500\n",
            "DMTS Score: 0.9824561403508771\n",
            "Majority Score: 0.9824561403508771\n",
            "Random Forest Score: 0.9473684210526315\n",
            "Adaboost Score: 0.9736842105263158\n",
            "-------------------------------------------------------------------\n",
            "Working on dataset German\n",
            "Finished trainining Decision_Tree\n",
            "Finished trainining Naive_Bayes\n",
            "Finished trainining Perceptron\n",
            "Finished trainining Logistic_500\n",
            "Finished trainining Logistic_400\n",
            "Finished trainining Logistic_300\n",
            "Finished trainining RandomForest_15\n",
            "Finished trainining RandomForest_50\n",
            "Finished trainining RandomForest_100\n",
            "Finished trainining SVM_rbf\n",
            "Finished trainining SVM_linear\n",
            "Finished trainining SVM_poly\n",
            "Finished trainining MLP_300\n",
            "Finished trainining MLP_400\n",
            "Finished trainining MLP_500\n",
            "Working on classifier Decision_Tree\n",
            "Working on classifier Naive_Bayes\n",
            "Working on classifier Perceptron\n",
            "Working on classifier Logistic_500\n",
            "Working on classifier Logistic_400\n",
            "Working on classifier Logistic_300\n",
            "Working on classifier RandomForest_15\n",
            "Working on classifier RandomForest_50\n",
            "Working on classifier RandomForest_100\n",
            "Working on classifier SVM_rbf\n",
            "Working on classifier SVM_linear\n",
            "Working on classifier SVM_poly\n",
            "Working on classifier MLP_300\n",
            "Working on classifier MLP_400\n",
            "Working on classifier MLP_500\n",
            "Done\n",
            "HMTS Score: 0.77\n",
            "Majority Score: 0.775\n",
            "Finished trainining Decision_Tree\n",
            "Finished trainining Naive_Bayes\n",
            "Finished trainining Perceptron\n",
            "Finished trainining Logistic_500\n",
            "Finished trainining Logistic_400\n",
            "Finished trainining Logistic_300\n",
            "Finished trainining RandomForest_15\n",
            "Finished trainining RandomForest_50\n",
            "Finished trainining RandomForest_100\n",
            "Finished trainining SVM_rbf\n",
            "Finished trainining SVM_linear\n",
            "Finished trainining SVM_poly\n",
            "Finished trainining MLP_300\n",
            "Finished trainining MLP_400\n",
            "Finished trainining MLP_500\n",
            "DMTS Score: 0.78\n",
            "Majority Score: 0.775\n",
            "Random Forest Score: 0.78\n",
            "Adaboost Score: 0.725\n",
            "-------------------------------------------------------------------\n",
            "Working on dataset Australian\n",
            "Finished trainining Decision_Tree\n",
            "Finished trainining Naive_Bayes\n",
            "Finished trainining Perceptron\n",
            "Finished trainining Logistic_500\n",
            "Finished trainining Logistic_400\n",
            "Finished trainining Logistic_300\n",
            "Finished trainining RandomForest_15\n",
            "Finished trainining RandomForest_50\n",
            "Finished trainining RandomForest_100\n",
            "Finished trainining SVM_rbf\n",
            "Finished trainining SVM_linear\n",
            "Finished trainining SVM_poly\n",
            "Finished trainining MLP_300\n",
            "Finished trainining MLP_400\n",
            "Finished trainining MLP_500\n",
            "Working on classifier Decision_Tree\n",
            "Working on classifier Naive_Bayes\n",
            "Working on classifier Perceptron\n",
            "Working on classifier Logistic_500\n",
            "Working on classifier Logistic_400\n",
            "Working on classifier Logistic_300\n",
            "Working on classifier RandomForest_15\n",
            "Working on classifier RandomForest_50\n",
            "Working on classifier RandomForest_100\n",
            "Working on classifier SVM_rbf\n",
            "Working on classifier SVM_linear\n",
            "Working on classifier SVM_poly\n",
            "Working on classifier MLP_300\n",
            "Working on classifier MLP_400\n",
            "Working on classifier MLP_500\n",
            "Done\n",
            "HMTS Score: 0.8623188405797102\n",
            "Majority Score: 0.8623188405797102\n",
            "Finished trainining Decision_Tree\n",
            "Finished trainining Naive_Bayes\n",
            "Finished trainining Perceptron\n",
            "Finished trainining Logistic_500\n",
            "Finished trainining Logistic_400\n",
            "Finished trainining Logistic_300\n",
            "Finished trainining RandomForest_15\n",
            "Finished trainining RandomForest_50\n",
            "Finished trainining RandomForest_100\n",
            "Finished trainining SVM_rbf\n",
            "Finished trainining SVM_linear\n",
            "Finished trainining SVM_poly\n",
            "Finished trainining MLP_300\n",
            "Finished trainining MLP_400\n",
            "Finished trainining MLP_500\n",
            "DMTS Score: 0.8623188405797102\n",
            "Majority Score: 0.8623188405797102\n",
            "Random Forest Score: 0.8623188405797102\n",
            "Adaboost Score: 0.8913043478260869\n",
            "-------------------------------------------------------------------\n",
            "Working on dataset Hill-Valley\n",
            "Finished trainining Decision_Tree\n",
            "Finished trainining Naive_Bayes\n",
            "Finished trainining Perceptron\n",
            "Finished trainining Logistic_500\n",
            "Finished trainining Logistic_400\n",
            "Finished trainining Logistic_300\n",
            "Finished trainining RandomForest_15\n",
            "Finished trainining RandomForest_50\n",
            "Finished trainining RandomForest_100\n",
            "Finished trainining SVM_rbf\n",
            "Finished trainining SVM_linear\n",
            "Finished trainining SVM_poly\n",
            "Finished trainining MLP_300\n",
            "Finished trainining MLP_400\n",
            "Finished trainining MLP_500\n",
            "Working on classifier Decision_Tree\n",
            "Working on classifier Naive_Bayes\n",
            "Working on classifier Perceptron\n",
            "Working on classifier Logistic_500\n",
            "Working on classifier Logistic_400\n",
            "Working on classifier Logistic_300\n",
            "Working on classifier RandomForest_15\n",
            "Working on classifier RandomForest_50\n",
            "Working on classifier RandomForest_100\n",
            "Working on classifier SVM_rbf\n",
            "Working on classifier SVM_linear\n",
            "Working on classifier SVM_poly\n",
            "Working on classifier MLP_300\n",
            "Working on classifier MLP_400\n",
            "Working on classifier MLP_500\n",
            "Done\n",
            "HMTS Score: 0.9300411522633745\n",
            "Majority Score: 0.9423868312757202\n",
            "Finished trainining Decision_Tree\n",
            "Finished trainining Naive_Bayes\n",
            "Finished trainining Perceptron\n",
            "Finished trainining Logistic_500\n",
            "Finished trainining Logistic_400\n",
            "Finished trainining Logistic_300\n",
            "Finished trainining RandomForest_15\n",
            "Finished trainining RandomForest_50\n",
            "Finished trainining RandomForest_100\n",
            "Finished trainining SVM_rbf\n",
            "Finished trainining SVM_linear\n",
            "Finished trainining SVM_poly\n",
            "Finished trainining MLP_300\n",
            "Finished trainining MLP_400\n",
            "Finished trainining MLP_500\n",
            "DMTS Score: 0.9506172839506173\n",
            "Majority Score: 0.9423868312757202\n",
            "Random Forest Score: 0.9794238683127572\n",
            "Adaboost Score: 0.9094650205761317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "7-H1zATnv7N3",
        "outputId": "ff77c3ea-bfdd-43d7-dadd-0b32ac3dc39a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-100f62972f2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
          ]
        }
      ]
    }
  ]
}
